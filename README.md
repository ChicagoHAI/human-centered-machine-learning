Human-Centered Machine Learning (Spring 2021)
============================

## Course staff
Insturctor: Chenhao Tan [contact](mailto:chenhao@uchicago.edu)      
Office hours: 2:30-3:30pm on Tuesday or by appointment


## Logistics

* Location and time: Tuesday and Thursday 1:00-2:20pm on Zoom
* [Syllabus](syllabus.md) (Must READ if you are taking the course)
* [Canvas](https://canvas.uchicago.edu/courses/34603)
* Zoom link (available on canvas)
* Ed (available on canvas)


Schedule
===========================

* Week 1: Introduction
	* Mar 30, Introduction  
	* Apr 1, [Ask not what AI can do, but what AI should do: Towards a Framework of Task Delegability](https://arxiv.org/abs/1902.03245). Brian Lubars and Chenhao Tan. NeurIPS 2019.  
	_Additional reading_:
		* [Human-centered Machine Learning: a Machine-in-the-loop Approach](https://medium.com/@ChenhaoTan/human-centered-machine-learning-a-machine-in-the-loop-approach-ed024db34fe7), Chenhao Tan.
		* [Human Perceptions on Moral Responsibility of AI: A Case Study in AI-Assisted Bail Decision-Making](https://arxiv.org/abs/2102.00625). Gabriel Lima, Nina Grgić-Hlača, Meeyoung Cha. CHI 2021.

* Week 2: You are not so Smart
	* Apr 6, [Human Decisions and Machine Predictions](https://academic.oup.com/qje/article/doi/10.1093/qje/qjx032/4095198/Human-Decisions-and-Machine-Predictions#).
	Jon Kleinberg, Himabindu Lakkaraju, Jure Leskovec, Jens Ludwig, Sendhil Mullainathan. 
Quarterly Journal of Economics, 2018.   
	* Apr 8, Telling More Than We Can Know: Verbal Reports on Mental Processes (pdf is available on CANVAS). Richard E. Nisbett and Timothy DeCamp Wilson. Psychological Review. 1977.   
	_Additional reading_:
		* [Judgment under Uncertainty: Heuristics and Biases. Amos Tversky and Daniel Kahneman](http://science.sciencemag.org/content/185/4157/1124), Science, 1974.
		* [Are Emily and Greg more employable than Lakisha and Jamal? A field experiment on labor market discrimination](https://www.aeaweb.org/articles?id=10.1257/0002828042002561). Marianne Bertrand and Sendhil Mullainathan. American Economic Review. 2004.
		* [An algorithmic approach to reducing unexplained pain disparities in underserved populations](https://www.nature.com/articles/s41591-020-01192-7). Emma Pierson, David M. Cutler, Jure Leskovec, Sendhil Mullainathan, and Ziad Obermeyer. Nature Medicine. 2021.
		* [Assessing Human Error Against a Benchmark of Perfection](https://arxiv.org/abs/1606.04956). Ashton Anderson, Jon Kleinberg, Sendhil Mullainathan. KDD 2016.
		* [Algorithm Aversion: People Erroneously Avoid Algorithms
After Seeing Them Err](http://opim.wharton.upenn.edu/risk/library/WPAF201410-AlgorthimAversion-Dietvorst-Simmons-Massey.pdf). Berkeley J. Dietvorst, Joseph P. Simmons, and Cade Massey, Journal of Experimental Psychology: General, 2014.
		* Podcast: [You are not so smart](https://youarenotsosmart.com/podcast/)
		* [The effect of wording on message propagation: Topic- and author-controlled natural experiments on Twitter](https://chenhaot.com/papers/wording-for-propagation.html). Chenhao Tan, Lillian Lee, Bo Pang. ACL 2014.   
		* Thinking, Fast and Slow. Daniel Kahneman. 2011.    
	<span style="color:red">**First proposal due on Apr 9**</span>

* Week 3: Machine Bias
	* Apr 13, [Dissecting racial bias in an algorithm used to manage the health of populations](https://science.sciencemag.org/content/366/6464/447.abstract). Ziad Obermeyer, Brian Powers, Christine Vogeli, Sendhil Mullainathan. Science. 2019.
	* Apr 15, [Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification](http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf). Joy Buolamwini and Timnit Gebru. FAccT 2018.  
	_Additional reading_:
		* [Language (Technology) is Power: A Critical Survey of "Bias" in NLP](https://arxiv.org/abs/2005.14050). Su Lin Blodgett, Solon Barocas, Hal Daumé III, Hanna Wallach. 2020.
		* [Discrimination in Online Ad Delivery](https://arxiv.org/pdf/1301.6822.pdf). Latanya Sweeney.  Communications of the ACM, 2013.
		* [Racial disparities in automated speech recognition](https://www.pnas.org/content/117/14/7684.short). Allison Koenecke, Andrew Nam, Emily Lake, Joe Nudell, Minnie Quartey, Zion Mengesha, Connor Toups, John R. Rickford, Dan Jurafsky, and Sharad Goel. PNAS. 2020.
		* [The Risk of Racial Bias in Hate Speech Detection](https://www.aclweb.org/anthology/P19-1163/). Maarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi, Noah A. Smith. ACL 2019.
		* [Gender bias in coreference resolution: Evaluation and debiasing methods](https://www.aclweb.org/anthology/N18-2003/). Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, Kai-Wei Chang. NAACL 2018.
		* [Men also like shopping: Reducing gender bias amplification using corpus-level constraints](https://arxiv.org/abs/1707.09457). Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, Kai-Wei Chang. EMNLP 2017.
		* [Man is to computer programmer as woman is to homemaker? debiasing word embeddings](https://arxiv.org/abs/1607.06520). Tolga Bolukbasi, Kai-Wei Chang, James Zou, Venkatesh Saligrama, Adam Kalai. NeurIPS 2016.
		

* Week 4: Accountability Framework
	* Apr 20, [Roles for Computing in Social Change](https://arxiv.org/abs/1912.04883). Rediet Abebe, Solon Barocas, Jon Kleinberg, Karen Levy, Manish Raghavan, David G. Robinson. FAccT 2020. 
	* Apr 22, [Model Cards for Model Reporting](https://dl.acm.org/doi/abs/10.1145/3287560.3287596). Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, Timnit Gebru. FAccT 2019.   
	_Additional reading_:
		* [Actionable Auditing: Investigating the Impact of Publicly Naming Biased Performance Results of Commercial AI Products](https://dl.acm.org/doi/10.1145/3306618.3314244). Inioluwa Deborah Raji and Joy Buolamwini. AIES 2019.
		* [Datasheets for datasets](https://arxiv.org/abs/1803.09010). Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé III, Kate Crawford. 2018.
		*  [European Union regulations on algorithmic decision-making and a "right to explanation"](https://arxiv.org/pdf/1606.08813.pdf). Bryce Goodman and Seth Flaxman. 
		* [Equality of opportunity in supervised learning](https://arxiv.org/abs/1610.02413). Moritz Hardt, Eric Price, Nathan Srebro. NeurIPS 2016.
		* [Inherent Trade-Offs in the Fair Determination of Risk Scores](https://arxiv.org/abs/1609.05807). Jon Kleinberg, Sendhil Mullainathan, Manish Raghavan. ITCS 2017.   
		* [Algorithmic decision making and the cost of fairness](https://arxiv.org/abs/1701.08230). Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, Aziz Huq. KDD 2017. 
		* [Fairness and Abstraction in Sociotechnical Systems](https://dl.acm.org/doi/10.1145/3287560.3287598). Andrew D. Selbst, danah boyd, Sorelle A. Friedler, Suresh Venkatasubramanian, Janet Vertesi. FAT* 2019.   
	<span style="color:red">**Second proposal due on Apr 23.**</span>
	

* Week 5: Machine-in-the-loop Interactions
	* Apr 27, [Guidelines for Human-AI Interaction](https://www.microsoft.com/en-us/research/uploads/prod/2019/01/Guidelines-for-Human-AI-Interaction-camera-ready.pdf). Saleema Amershi, Dan Weld, Mihaela Vorvoreanu, Adam Fourney, Besmira Nushi, Penny Collisson,
Jina Suh, Shamsi Iqbal, Paul N. Bennett, Kori Inkpen, Jaime Teevan, Ruth Kikin-Gil, and Eric Horvitz. CHI 2019.
	* Apr 29, [Towards A Rigorous Science of Interpretable Machine Learning](https://arxiv.org/abs/1702.08608). Finale Doshi-Velez and Been Kim.   
	_Additional reading_: 
		* [Principles of Mixed-Initiative User Interfaces](http://erichorvitz.com/chi99horvitz.pdf). Eric Horvitz. In Proceedings of CHI, 1999. 
		* [Questioning the AI: informing design practices for explainable AI user experiences](https://arxiv.org/abs/2001.02478). Q. Vera Liao, Daniel Gruen, Sarah Miller. CHI 2020.
		* [Human-centered artificial intelligence: Reliable, safe & trustworthy](https://arxiv.org/abs/2002.04087). Ben Shneiderman. International Journal of Human-Computer Interaction. 2020.
		* [Trends and Trajectories for Explainable, Accountable and Intelligible Systems: An HCI Research Agenda](https://dl.acm.org/doi/10.1145/3173574.3174156).  Ashraf Abdul, Jo Vermeulen, Danding Wang, Brian Y. Lim, Mohan Kankanhalli. CHI 2018.
		* [Interpretable machine learning: definitions, methods, and applications](https://arxiv.org/abs/1901.04592). W. James Murdoch, Chandan Singh, Karl Kumbier, Reza Abbasi-Asl, Bin Yu. PNAS 2019.
		* [Natural Language Translation at the Intersection of AI and HCI](https://dl.acm.org/citation.cfm?id=2798086). Spence Gree, Jeffrey Heer, Christopher D. Manning. Queue, 2015.    
		* [A Review of User Interface Design for Interactive Machine Learning](https://www.repository.cam.ac.uk/bitstream/handle/1810/274032/TIIS_Special_Issue_IML_Survey.pdf). John J. Dudley and Per Ola Kristensson. ACM Transactions on Interactive Intelligent Systems. 2018.
		* [Beyond binary choices: Integrating individual and social creativity](https://www.sciencedirect.com/science/article/pii/S1071581905000479). Gerhard Fischer, Elisa Giaccardi, Hal Eden, Masanori Sugimoto, Yunwen Ye. International Journal of Human-Computer Studies, 2005.  
		* [The Mythos of Model Interpretability](https://arxiv.org/abs/1606.03490), Zachary C. Lipton.  
		* [A Human-Centered Agenda for Intelligible Machine Learning](http://www.jennwv.com/papers/intel-chapter.pdf). Jennifer Wortman Vaughan and Hanna Wallach.  
		* [Who is the “Human” in Human-Centered Machine Learning: The Case of Predicting Mental Health from Social Media](http://steviechancellor.com/wp-content/uploads/2019/09/HCML-CSCW-2019.pdf). Stevie Chancellor, Eric P.S Baumer, and Munmun De Choudhury. CSCW 2019.
		* [Human Centered Systems in the Perspective of Organizational and Social Informatics](https://scholarworks.iu.edu/dspace/bitstream/handle/2022/1798/wp97-04B.html). Rob Kling and Leigh Star. 1997.   
		<span style="color:red">**Team formation due on Apr 30.**</span>

* Week 6: Feature-based Explanations
	* May 4, [Why should I trust you?: Explaining the Predictions of Any Classifier](https://arxiv.org/abs/1602.04938). Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin. KDD 2016.
	* May 6, [Axiomatic attribution for deep networks](http://proceedings.mlr.press/v70/sundararajan17a.html). Mukund Sundararajan, Ankur Taly, Qiqi Yan. ICML 2017.  
	_Additional reading_:
		* [A Unified Approach to Interpreting Model Predictions](https://arxiv.org/abs/1705.07874). Scott Lundberg, Su-In Lee. NeurIPS 2017.
		* [Attention is not explanations](https://arxiv.org/abs/1902.10186) Sarthak Jain, Byron C. Wallace. NAACL 2019.
		* [Attention is not not explanations](https://arxiv.org/abs/1908.04626). Sarah Wiegreffe, Yuval Pinter. EMNLP 2019.
		* [Is Attention Interpretable?](https://arxiv.org/abs/1906.03731). Sofia Serrano, Noah A. Smith. ACL 2019.
		* [Many Faces of Feature Importance: Comparing Built-in and Post-hoc Feature Importance in Text Classification](https://arxiv.org/abs/1910.08534). Vivian Lai, Jon Z. Cai, Chenhao Tan. EMNLP 2019.
		* [DeepXplore: Automated Whitebox Testing of Deep Learning Systems](https://arxiv.org/abs/1705.06640), Pei, Cao, Yang and Jana. In Proceedings of SOSP, 2017.
		* [Rationalizing Neural Predictions](https://people.csail.mit.edu/taolei/papers/emnlp16_rationale.pdf), Tao Lei, Regina Barzilay and Tommi Jaakkola. In Proceedings of EMNLP, 2016.
		* [Learning Explanatory Rules from Noisy Data](https://arxiv.org/abs/1711.04574). Richard Evans, Edward Grefenstette. Journal of Artificial Intelligence Research, 2018.
		* [Grad-cam: Visual explanations from deep networks via gradient-based localization](https://arxiv.org/abs/1610.02391). Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, Dhruv Batra. ICCV 2017.
		* [Towards Faithfully Interpretable NLP Systems: How should we define and evaluate faithfulness?](https://arxiv.org/abs/2004.03685) Alon Jacovi, Yoav Goldberg. ACL 2020.
		* [Evaluating and Characterizing Human Rationales](https://arxiv.org/abs/2010.04736). Samuel Carton, Anirudh Rathore, Chenhao Tan. EMNLP 2020.
		* [Network Dissection: Quantifying Interpretability of Deep Visual Representations](http://netdissect.csail.mit.edu/final-network-dissection.pdf). David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, Antonio Torralba. In Proceedings of CVPR 2017.    
	

	
* Week 7: Example-based explanations
	* May 11, [Deep Weighted Averaging Classifiers](https://arxiv.org/abs/1811.02579). Dallas Card, Michael Zhang, Noah A. Smith. FAccT 2019.  
	* May 13, [Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GDPR](https://arxiv.org/abs/1711.00399). Sandra Wachter, Brent Mittelstadt, Chris Russell.
	_Additional reading_:
		* [Examples are not Enough, Learn to Criticize!
Criticism for Interpretability](http://papers.nips.cc/paper/6300-examples-are-not-enough-learn-to-criticize-criticism-for-interpretability.pdf). Been Kim, Rajiv Khanna, Oluwasanmi Koyejo. NeurIPS 2016.
		* [Case-based explanation of non-case-based learning methods](https://www.ncbi.nlm.nih.gov/pubmed/10566351). Rich Caruana, Hooshang Kangarloo, John David
N. Dionisio, Usha Sinha, David Johnson. AMIA 1999.
		* [How Case Based Reasoning Explained Neural Networks: An XAI Survey of Post-Hoc Explanation-by-Example in ANN-CBR Twins](https://arxiv.org/abs/1905.07186). Mark T Keane, Eoin M Kenny. ICCBR 2019.
		* [Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust Deep Learning](https://arxiv.org/abs/1803.04765). Nicolas Papernot, Patrick McDaniel. 
		* [Interactive and Interpretable Machine Learning Models for Human Machine Collaboration](http://people.csail.mit.edu/beenkim/papers/BKimPhDThesis.pdf), Been Kim, PhD thesis.    
		* [Explaining Machine Learning Classifiers through Diverse Counterfactual Explanations](https://arxiv.org/abs/1905.07697). Ramaravind Kommiya Mothilal, Amit Sharma, Chenhao Tan. FAT* 2020.    
		* [The Hidden Assumptions Behind Counterfactual Explanations and Principal Reasons](https://arxiv.org/abs/1912.04930). Solon Barocas, Andrew D. Selbst, Manish Raghavan. FAT* 2020.
		* [Efficient Search for Diverse Coherent Explanations](https://arxiv.org/abs/1901.04909). Chris Russell. FAT* 2019.


* Week 8: Human-AI interaction --- Decision Making
	* May 18, [On Human Predictions with Explanations and Predictions of Machine Learning Models: A Case Study on Deception Detection](https://arxiv.org/abs/1811.07901). Vivian Lai, Chenhao Tan. FAccT 2019.
	* May 20, [The Principles and Limits of Algorithm-in-the-Loop Decision Making](https://www.benzevgreen.com/wp-content/uploads/2019/09/19-cscw.pdf). Ben Green, Yiling Chen. CSCW 2019.        
	_Additional reading_: 
		* [The limits of human predictions of recidivism](https://advances.sciencemag.org/content/6/7/eaaz0652). Zhiyuan “Jerry” Lin, Jongbin Jung, Sharad Goel, Jennifer Skeem. Science Advances 2020.
		* [Human Evaluation of Models Built for Interpretability](https://arxiv.org/abs/1902.00006). Isaac Lage, Emily Chen, Jeffrey He, Menaka Narayanan, Been Kim, Samuel Gershman and Finale Doshi-Velez. HCOMP 2019.
		* [Proxy Tasks and Subjective Measures Can Be Misleading in Evaluating Explainable AI Systems](http://www.eecs.harvard.edu/~kgajos/papers/2020/bucinca20proxy.pdf). Zana Buçinca, Phoebe Lin, Krzysztof Z. Gajos, and Elena L. Glassman. IUI 2020.
		* [Does the Whole Exceed its Parts? The Effect of AI Explanations on Complementary Team Performance](https://aiweb.cs.washington.edu/ai/pubs/bansal-chi21.pdf). Gagan Bansal\*, Tongshuang Wu\*, Joyce Zhou, Raymond Fok, Besmira Nushi, Ece Kamar, Marco Tulio Ribeiro, Daniel S. Weld. CHI 2021.
		* [Prediction Policy Problems](https://www.aeaweb.org/articles?id=10.1257/aer.p20151023). Jon Kleinberg, Jens Ludwig, Sendhil Mullainathan, Ziad Obermeyer. American Economic Review, 2015.  
		* [The Tyranny of Data? The Bright and Dark Sides of Data-Driven Decision-Making for Social Good](https://link.springer.com/chapter/10.1007/978-3-319-54024-5_1). Bruno Lepri, Jacopo Staiano, David Sangokoya, Emmanuel Letouzé, and Nuria Oliver. Transparent Data Mining for Big and Small Data, 2017.  
		* [Predicting the knowledge–recklessness distinction in the human brain](http://www.pnas.org/content/114/12/3222.full). Iris Vilares, Michael J. Wesley, Woo-Young Ahn, Richard J. Bonnie, Morris Hoffman, Owen D. Jones, Stephen J. Morse, Gideon Yaffe, Terry Lohrenz, and P. Read Montague. PNAS, 2016. 

* Week 9: Human-AI interaction --- Trust
	* May 25, [Trust in automation: Designing for appropriate reliance](https://journals.sagepub.com/doi/10.1518/hfes.46.1.50_30392). John Lee and Katrina See. Human factors, 2004.
	* May 27, [Impact of a deep learning assistant on the histopathologic classification of liver cancer](https://www.nature.com/articles/s41746-020-0232-8). Amirhossein Kiani, Bora Uyumazturk, Pranav Rajpurkar, Alex Wang, Rebecca Gao, Erik Jones, Yifan Yu, Curtis P. Langlotz, Robyn L. Ball, Thomas J. Montine, Brock A. Martin, Gerald J. Berry, Michael G. Ozawa, Florette K. Hazard, Ryanne A. Brown, Simon B. Chen, Mona Wood, Libby S. Allard, Lourdes Ylagan, Andrew Y. Ng & Jeanne Shen. npj Digital Medicine, 2020.    
	_Additional reading_:
		* [To Trust or to Think: Cognitive Forcing Functions CanReduce Overreliance on AI in AI-assisted Decision-making](https://arxiv.org/abs/2102.09692). Zana Buçinca, Maja Barbara Malaya, Krzysztof Z. Gajos. CSCW 2021.
		* [Formalizing Trust in Artificial Intelligence: Prerequisites, Causes and Goals of Human Trust in AI](https://arxiv.org/abs/2010.07487). Alon Jacovi, Ana Marasović, Tim Miller, Yoav Goldberg. FAccT 2021.
		* [Let Me Explain: Impact of Personal and Impersonal Explanations on Trust in Recommender Systems](https://dl.acm.org/doi/10.1145/3290605.3300717). Johannes Kunkel, Tim Donkers, Lisa Michael, Catalin-Mihai Barbu, and Jürgen Ziegler. CHI 2019.
		* [Understanding the Effect of Accuracy on Trust in
Machine Learning Models](http://mingyin.org/CHI-19/accuracy.pdf). Ming Yin, Jennifer Wortman Vaughan, and Hanna Wallach. CHI 2019.  
		


* Week 10: Final Project Presentation  
	* TBD, presentations (could be different from normal class time).   
<span style="color:red">**Final project report due on Jun 1.**</span>
