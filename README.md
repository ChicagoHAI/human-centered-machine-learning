Human-Centered Machine Learning (Spring 2022)
============================

## Course staff
Insturctor: Chenhao Tan [contact](mailto:chenhao@uchicago.edu)      
Office hours: 1:30-1:55pm at Searle 215 on Tuesday and Thursdays or by appointment


## Logistics

* Location and time: Tuesday and Thursday 2:00-3:20 at Ryerson 277
* [Syllabus](syllabus.md) (Must READ if you are taking the course)
* [Canvas](https://canvas.uchicago.edu/courses/42242)
* Ed (available on canvas)


Schedule
===========================

* Week 1: Introduction
	* Mar 21, Introduction  
	* Mar 23, [Ask not what AI can do, but what AI should do: Towards a Framework of Task Delegability](https://arxiv.org/abs/1902.03245). Brian Lubars and Chenhao Tan. NeurIPS 2019.  
	_Additional reading_:
		* [Human-centered Machine Learning: a Machine-in-the-loop Approach](https://medium.com/@ChenhaoTan/human-centered-machine-learning-a-machine-in-the-loop-approach-ed024db34fe7), Chenhao Tan.
		* [Human Perceptions on Moral Responsibility of AI: A Case Study in AI-Assisted Bail Decision-Making](https://arxiv.org/abs/2102.00625). Gabriel Lima, Nina Grgić-Hlača, Meeyoung Cha. CHI 2021.

* Week 2: The Role of AI
	* Mar 28, [Roles for Computing in Social Change](https://arxiv.org/abs/1912.04883). Rediet Abebe, Solon Barocas, Jon Kleinberg, Karen Levy, Manish Raghavan, David G. Robinson. FAccT 2020.  
	* Mar 30, [Beyond prediction: Using big data for policy problems](https://www.science.org/doi/full/10.1126/science.aal4321). Susan Athey. Science 2017.      
	_Additional reading_:
		* [Prediction Policy Problems](https://www-aeaweb-org.proxy.uchicago.edu/articles?id=10.1257/aer.p20151023). Jon Kleinberg, Jens Ludwig, Sendhil Mullainathan, Ziad Obermeyer. American Economic Review 2015.
		* [Human Decisions and Machine Predictions](https://academic.oup.com/qje/article/doi/10.1093/qje/qjx032/4095198/Human-Decisions-and-Machine-Predictions#).
	Jon Kleinberg, Himabindu Lakkaraju, Jure Leskovec, Jens Ludwig, Sendhil Mullainathan. 
Quarterly Journal of Economics, 2018. 
		* [How AI fails us](https://ethics.harvard.edu/files/center-for-ethics/files/howai_fails_us_2.pdf). Divya Siddarth, Daron Acemoglu, Danielle Allen, Kate Crawford, James Evans, Michael Jordan, E. Glen Weyl. 2021  
		* [Understanding Human Intelligence through Human Limitations](https://arxiv.org/abs/2009.14050). Thomas L. Griffiths. Trends in Cognitive Science. 2020.
		* [Judgment under Uncertainty: Heuristics and Biases. Amos Tversky and Daniel Kahneman](http://science.sciencemag.org/content/185/4157/1124), Science, 1974.
		* [Are Emily and Greg more employable than Lakisha and Jamal? A field experiment on labor market discrimination](https://www.aeaweb.org/articles?id=10.1257/0002828042002561). Marianne Bertrand and Sendhil Mullainathan. American Economic Review. 2004.
		* [An algorithmic approach to reducing unexplained pain disparities in underserved populations](https://www.nature.com/articles/s41591-020-01192-7). Emma Pierson, David M. Cutler, Jure Leskovec, Sendhil Mullainathan, and Ziad Obermeyer. Nature Medicine. 2021.
		* [Assessing Human Error Against a Benchmark of Perfection](https://arxiv.org/abs/1606.04956). Ashton Anderson, Jon Kleinberg, Sendhil Mullainathan. KDD 2016.
		* [Algorithm Aversion: People Erroneously Avoid Algorithms
After Seeing Them Err](http://opim.wharton.upenn.edu/risk/library/WPAF201410-AlgorthimAversion-Dietvorst-Simmons-Massey.pdf). Berkeley J. Dietvorst, Joseph P. Simmons, and Cade Massey, Journal of Experimental Psychology: General, 2014.
		* Podcast: [You are not so smart](https://youarenotsosmart.com/podcast/)
		* [The effect of wording on message propagation: Topic- and author-controlled natural experiments on Twitter](https://chenhaot.com/papers/wording-for-propagation.html). Chenhao Tan, Lillian Lee, Bo Pang. ACL 2014.   
		* Thinking, Fast and Slow. Daniel Kahneman. 2011.    
	<span style="color:red">**First proposal due on Mar 31**</span>

* Week 3: Exposing biases
	* Apr 4, [Dissecting racial bias in an algorithm used to manage the health of populations](https://science.sciencemag.org/content/366/6464/447.abstract). Ziad Obermeyer, Brian Powers, Christine Vogeli, Sendhil Mullainathan. Science. 2019.
	* Apr 6, [Underdiagnosis bias of artificial intelligence algorithms applied to chest radiographs in under-served patient populations](https://www.nature.com/articles/s41591-021-01595-0). Laleh Seyyed-Kalantari, Haoran Zhang, Matthew B. A. McDermott, Irene Y. Chen, and  Marzyeh Ghassemi. Natural Medicine. 2021.    
	_Additional reading_:
		* [An algorithmic approach to reducing unexplained pain disparities in underserved populations](https://www.nature.com/articles/s41591-020-01192-7). Emma Pierson, David M. Cutler, Jure Leskovec, Sendhil Mullainathan & Ziad Obermeyer. Nature Medicine 2021.
		* [Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification](http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf). Joy Buolamwini and Timnit Gebru. FAccT 2018.  
		* [Discrimination in Online Ad Delivery](https://arxiv.org/pdf/1301.6822.pdf). Latanya Sweeney.  Communications of the ACM, 2013.
		* [Racial disparities in automated speech recognition](https://www.pnas.org/content/117/14/7684.short). Allison Koenecke, Andrew Nam, Emily Lake, Joe Nudell, Minnie Quartey, Zion Mengesha, Connor Toups, John R. Rickford, Dan Jurafsky, and Sharad Goel. PNAS. 2020.
		* [The Risk of Racial Bias in Hate Speech Detection](https://www.aclweb.org/anthology/P19-1163/). Maarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi, Noah A. Smith. ACL 2019.
		* [Gender bias in coreference resolution: Evaluation and debiasing methods](https://www.aclweb.org/anthology/N18-2003/). Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, Kai-Wei Chang. NAACL 2018.
		* [Men also like shopping: Reducing gender bias amplification using corpus-level constraints](https://arxiv.org/abs/1707.09457). Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, Kai-Wei Chang. EMNLP 2017.
		* [Man is to computer programmer as woman is to homemaker? debiasing word embeddings](https://arxiv.org/abs/1607.06520). Tolga Bolukbasi, Kai-Wei Chang, James Zou, Venkatesh Saligrama, Adam Kalai. NeurIPS 2016.
		* [Language (Technology) is Power: A Critical Survey of "Bias" in NLP](https://arxiv.org/abs/2005.14050). Su Lin Blodgett, Solon Barocas, Hal Daumé III, Hanna Wallach. 2020.
		* * [Post hoc Explanations may be Ineffective for Detecting Unknown Spurious Correlation]([https://arxiv.org/abs/2005.14050](https://openreview.net/forum?id=xNOVfCCvDpM)). Julius Adebayo, Michael Muelly, Harold Abelson, Been Kim. 2022.

		
* Week 4: Human-AI interaction --- Decision Making
	* Apr 11, [On Human Predictions with Explanations and Predictions of Machine Learning Models: A Case Study on Deception Detection](https://arxiv.org/abs/1811.07901). Vivian Lai, Chenhao Tan. FAccT 2019.
	* Apr 13,  [Understanding the Effect of Out-of-distribution Examples and Interactive Explanations on Human-AI Decision Making](https://arxiv.org/pdf/2101.05303.pdf). Han Liu, Vivian Lai, and Chenhao Tan. CSCW 2021.        
	_Additional reading_: 
		* [Towards a Science of Human-AI Decision Making: A Survey of Empirical Studies](https://arxiv.org/abs/2112.11471). Vivian Lai, Chacha Chen, Alison Smith-Renner, Q. Vera Liao, and Chenhao Tan. 2021.
		* [The Principles and Limits of Algorithm-in-the-Loop Decision Making](https://www.benzevgreen.com/wp-content/uploads/2019/09/19-cscw.pdf). Ben Green, Yiling Chen. CSCW 2019.
		* [Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making](https://dl-acm-org.proxy.uchicago.edu/doi/abs/10.1145/3351095.3372852). Yunfeng Zhang, Q. Vera Liao, Rachel K. E. Bellamy. FAccT 2020.
		* [The limits of human predictions of recidivism](https://advances.sciencemag.org/content/6/7/eaaz0652). Zhiyuan “Jerry” Lin, Jongbin Jung, Sharad Goel, Jennifer Skeem. Science Advances 2020.
		* [Human Evaluation of Models Built for Interpretability](https://arxiv.org/abs/1902.00006). Isaac Lage, Emily Chen, Jeffrey He, Menaka Narayanan, Been Kim, Samuel Gershman and Finale Doshi-Velez. HCOMP 2019.
		* [Proxy Tasks and Subjective Measures Can Be Misleading in Evaluating Explainable AI Systems](http://www.eecs.harvard.edu/~kgajos/papers/2020/bucinca20proxy.pdf). Zana Buçinca, Phoebe Lin, Krzysztof Z. Gajos, and Elena L. Glassman. IUI 2020.
		* [Does the Whole Exceed its Parts? The Effect of AI Explanations on Complementary Team Performance](https://aiweb.cs.washington.edu/ai/pubs/bansal-chi21.pdf). Gagan Bansal\*, Tongshuang Wu\*, Joyce Zhou, Raymond Fok, Besmira Nushi, Ece Kamar, Marco Tulio Ribeiro, Daniel S. Weld. CHI 2021.
		* [The Tyranny of Data? The Bright and Dark Sides of Data-Driven Decision-Making for Social Good](https://link.springer.com/chapter/10.1007/978-3-319-54024-5_1). Bruno Lepri, Jacopo Staiano, David Sangokoya, Emmanuel Letouzé, and Nuria Oliver. Transparent Data Mining for Big and Small Data, 2017.  
		* [Predicting the knowledge–recklessness distinction in the human brain](http://www.pnas.org/content/114/12/3222.full). Iris Vilares, Michael J. Wesley, Woo-Young Ahn, Richard J. Bonnie, Morris Hoffman, Owen D. Jones, Stephen J. Morse, Gideon Yaffe, Terry Lohrenz, and P. Read Montague. PNAS, 2016. 
		* [The effectiveness of feature attribution methods and its correlation with automatic evaluation scores](http://www.pnas.org/content/114/12/3222.full). Giang Nguyen, Daeyoung Kim, and Anh Nguyen. NeurIPS, 2021.
		* [What I Cannot Predict, I Do Not Understand: A Human-Centered Evaluation Framework for Explainability Methods](https://arxiv.org/abs/2112.04417). Julien Colin, Thomas Fel, Remi Cadene, Thomas Serre. NeurIPS, 2022. 
		* [HIVE: Evaluating the Human Interpretability of Visual Explanations](https://arxiv.org/abs/2112.03184). Sunnie S. Y. Kim, Nicole Meister, Vikram V. Ramaswamy, Ruth Fong, Olga Russakovsky. ECCV, 2022.
		* [Visual correspondence-based explanations improve AI robustness and human-AI team accuracy](https://arxiv.org/abs/2208.00780). Giang Nguyen, Mohammad Reza Taesiri, Anh Nguyen. NeurIPS, 2022. 
	<span style="color:red">**Second proposal due on Apr 14.**</span>

* Week 5: Conditional delegation & learning to defer
	* Apr 18, A special treat; we will discuss two papers. [Human-AI Collaboration via Conditional Delegation: A Case Study of Content Moderation](https://arxiv.org/pdf/2204.11788.pdf). Vivian Lai, Samuel Carton, Rajat Bhatnagar, Q. Vera Liao, Yunfeng Zhang, and Chenhao Tan. CHI 2022, and [To Trust or to Think: Cognitive Forcing Functions Can
Reduce Overreliance on AI in AI-assisted Decision-making](https://arxiv.org/abs/2102.09692). Zana Buçinca, Maja Barbara Malaya, Krzysztof Z. Gajos. CSCW 2021.
	* Apr 20, [Predict Responsibly: Improving Fairness and Accuracy by Learning to Defer](https://proceedings.neurips.cc/paper/2018/hash/09d37c08f7b129e96277388757530c72-Abstract.html). David Madras, Toni Pitassi, Richard Zemel. NeurIPS 2018.
	_Additional reading_:
		* [Impact of a deep learning assistant on the histopathologic classification of liver cancer](https://www.nature.com/articles/s41746-020-0232-8). Amirhossein Kiani, Bora Uyumazturk, Pranav Rajpurkar, Alex Wang, Rebecca Gao, Erik Jones, Yifan Yu, Curtis P. Langlotz, Robyn L. Ball, Thomas J. Montine, Brock A. Martin, Gerald J. Berry, Michael G. Ozawa, Florette K. Hazard, Ryanne A. Brown, Simon B. Chen, Mona Wood, Libby S. Allard, Lourdes Ylagan, Andrew Y. Ng & Jeanne Shen. npj Digital Medicine, 2020.
		* [Trust in automation: Designing for appropriate reliance](https://journals.sagepub.com/doi/10.1518/hfes.46.1.50_30392). John Lee and Katrina See. Human factors, 2004.
		* [Formalizing Trust in Artificial Intelligence: Prerequisites, Causes and Goals of Human Trust in AI](https://arxiv.org/abs/2010.07487). Alon Jacovi, Ana Marasović, Tim Miller, Yoav Goldberg. FAccT 2021.
		* [Let Me Explain: Impact of Personal and Impersonal Explanations on Trust in Recommender Systems](https://dl.acm.org/doi/10.1145/3290605.3300717). Johannes Kunkel, Tim Donkers, Lisa Michael, Catalin-Mihai Barbu, and Jürgen Ziegler. CHI 2019.
		* [Understanding the Effect of Accuracy on Trust in
Machine Learning Models](http://mingyin.org/CHI-19/accuracy.pdf). Ming Yin, Jennifer Wortman Vaughan, and Hanna Wallach. CHI 2019.     
	<span style="color:red">**Project choice and team formation due on Apr 21.**</span> 

* Week 6: Case-based decision support
	* Apr 25, [Learning Human-Compatible Representations for Case-Based Decision Support](https://arxiv.org/abs/2303.04809). Han Liu, Yizhou Tian, Chacha Chen, Shi Feng, Yuxin Chen, and Chenhao Tan. ICLR 2023.
	* Apr 27, [Teaching Humans When To Defer to a Classifier via Exemplars](https://arxiv.org/abs/2111.11297). Hussein Mozannar, Arvind Satyanarayan, David Sontag. AAAI 2022.    
	_Additional reading_:
		* [This Looks Like That: Deep Learning for Interpretable Image Recognition](https://arxiv.org/abs/1806.10574). Chaofan Chen, Oscar Li, Chaofan Tao, Alina Jade Barnett, Jonathan Su, Cynthia Rudin. NeurIPS 2019.
		* [Beta Shapley: a Unified and Noise-reduced Data Valuation Framework for Machine Learning](https://arxiv.org/pdf/2110.14049.pdf). Yongchan Kwon, James Zou.  AISTATS 2022.
		* [The Use and Misuse of Counterfactuals in Ethical Machine Learning](https://dl.acm.org/doi/abs/10.1145/3442188.3445886). Atoosa Kasirzadeh, Andrew Smart. FAccT 2021.
		* [Data Shapley: Equitable Data Valuation for Machine Learning](https://arxiv.org/abs/1904.02868). Amirata Ghorbani, James Zou. ICML 2019.
		* [Understanding Black-box Predictions via Influence Functions](http://proceedings.mlr.press/v70/koh17a). Pang Wei Koh, Percy Liang. ICML 2017.
		* [Examples are not Enough, Learn to Criticize!
Criticism for Interpretability](http://papers.nips.cc/paper/6300-examples-are-not-enough-learn-to-criticize-criticism-for-interpretability.pdf). Been Kim, Rajiv Khanna, Oluwasanmi Koyejo. NeurIPS 2016.
		* [Case-based explanation of non-case-based learning methods](https://www.ncbi.nlm.nih.gov/pubmed/10566351). Rich Caruana, Hooshang Kangarloo, John David
N. Dionisio, Usha Sinha, David Johnson. AMIA 1999.
		* [How Case Based Reasoning Explained Neural Networks: An XAI Survey of Post-Hoc Explanation-by-Example in ANN-CBR Twins](https://arxiv.org/abs/1905.07186). Mark T Keane, Eoin M Kenny. ICCBR 2019.
		* [Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust Deep Learning](https://arxiv.org/abs/1803.04765). Nicolas Papernot, Patrick McDaniel. 
		* [Interactive and Interpretable Machine Learning Models for Human Machine Collaboration](http://people.csail.mit.edu/beenkim/papers/BKimPhDThesis.pdf), Been Kim, PhD thesis.    
		* [Explaining Machine Learning Classifiers through Diverse Counterfactual Explanations](https://arxiv.org/abs/1905.07697). Ramaravind Kommiya Mothilal, Amit Sharma, Chenhao Tan. FAccT 2020.    
		* [The Hidden Assumptions Behind Counterfactual Explanations and Principal Reasons](https://arxiv.org/abs/1912.04930). Solon Barocas, Andrew D. Selbst, Manish Raghavan. FAT* 2020.
		* [Efficient Search for Diverse Coherent Explanations](https://arxiv.org/abs/1901.04909). Chris Russell. FAccT 2019.
		* [Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GDPR](https://arxiv.org/abs/1711.00399). Sandra Wachter, Brent Mittelstadt, Chris Russell. 2017.  
		* [Data valuation for medical imaging using Shapley value and application to a large-scale chest X-ray dataset](https://www.nature.com/articles/s41598-021-87762-2). Siyi Tang, Amirata Ghorbani, R. Yamashita, S. Rehman, Jared Dunnmon, James Zou, Daniel Rubin. Scientific Reports (2021).

* Week 7: Promising directions of explanations
	* May 2, [Causal Thinking in Judgments](https://onlinelibrary.wiley.com/doi/epdf/10.1002/9781118468333.ch21). Reid Hastie. 2015.
	* May 4, [Explainable AI is Dead, Long Live Explainable AI! Hypothesis-driven decision support](https://arxiv.org/abs/2302.12389). Tim Miller. 2023.
	_Additional reading_:
		* [Towards A Rigorous Science of Interpretable Machine Learning](https://arxiv.org/abs/1702.08608). Finale Doshi-Velez and Been Kim.  
		* [Principles of Mixed-Initiative User Interfaces](http://erichorvitz.com/chi99horvitz.pdf). Eric Horvitz. In Proceedings of CHI, 1999. 
		* [Guidelines for Human-AI Interaction](https://www.microsoft.com/en-us/research/uploads/prod/2019/01/Guidelines-for-Human-AI-Interaction-camera-ready.pdf). Saleema Amershi, Dan Weld, Mihaela Vorvoreanu, Adam Fourney, Besmira Nushi, Penny Collisson,
Jina Suh, Shamsi Iqbal, Paul N. Bennett, Kori Inkpen, Jaime Teevan, Ruth Kikin-Gil, and Eric Horvitz. CHI 2019.
		* [Questioning the AI: informing design practices for explainable AI user experiences](https://arxiv.org/abs/2001.02478). Q. Vera Liao, Daniel Gruen, Sarah Miller. CHI 2020.
		* [Human-centered artificial intelligence: Reliable, safe & trustworthy](https://arxiv.org/abs/2002.04087). Ben Shneiderman. International Journal of Human-Computer Interaction. 2020.
		* [Trends and Trajectories for Explainable, Accountable and Intelligible Systems: An HCI Research Agenda](https://dl.acm.org/doi/10.1145/3173574.3174156).  Ashraf Abdul, Jo Vermeulen, Danding Wang, Brian Y. Lim, Mohan Kankanhalli. CHI 2018.
		* [Interpretable machine learning: definitions, methods, and applications](https://arxiv.org/abs/1901.04592). W. James Murdoch, Chandan Singh, Karl Kumbier, Reza Abbasi-Asl, Bin Yu. PNAS 2019.
		* [Natural Language Translation at the Intersection of AI and HCI](https://dl.acm.org/citation.cfm?id=2798086). Spence Gree, Jeffrey Heer, Christopher D. Manning. Queue, 2015.    
		* [A Review of User Interface Design for Interactive Machine Learning](https://www.repository.cam.ac.uk/bitstream/handle/1810/274032/TIIS_Special_Issue_IML_Survey.pdf). John J. Dudley and Per Ola Kristensson. ACM Transactions on Interactive Intelligent Systems. 2018.
		* [Beyond binary choices: Integrating individual and social creativity](https://www.sciencedirect.com/science/article/pii/S1071581905000479). Gerhard Fischer, Elisa Giaccardi, Hal Eden, Masanori Sugimoto, Yunwen Ye. International Journal of Human-Computer Studies, 2005.  
		* [The Mythos of Model Interpretability](https://arxiv.org/abs/1606.03490), Zachary C. Lipton.  
		* [A Human-Centered Agenda for Intelligible Machine Learning](http://www.jennwv.com/papers/intel-chapter.pdf). Jennifer Wortman Vaughan and Hanna Wallach.  
		* [Who is the “Human” in Human-Centered Machine Learning: The Case of Predicting Mental Health from Social Media](http://steviechancellor.com/wp-content/uploads/2019/09/HCML-CSCW-2019.pdf). Stevie Chancellor, Eric P.S Baumer, and Munmun De Choudhury. CSCW 2019.
		* [Human Centered Systems in the Perspective of Organizational and Social Informatics](https://scholarworks.iu.edu/dspace/bitstream/handle/2022/1798/wp97-04B.html). Rob Kling and Leigh Star. 1997.   
		* [The false hope of current approaches to explainable artificial intelligence in health care](https://www.sciencedirect.com/science/article/pii/S2589750021002089).    
	<span style="color:red">**Project progress report due on May 5.**</span>
	
* Week 8: Alignment through feedback
	* May 9, [Learning to summarize from human feedback](https://arxiv.org/abs/2009.01325). Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, Paul Christiano. NeurIPS 2020.
	* May 11, [Constitutional AI: Harmlessness from AI Feedback](https://arxiv.org/abs/2212.08073). Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, Jared Kaplan. 2023.     
	_Additional reading_: 
		* [Deep reinforcement learning from human preferences](https://arxiv.org/abs/1706.03741). Paul Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, Dario Amodei. 2017.
		* [Scalable agent alignment via reward modeling: a research direction](https://arxiv.org/abs/1811.07871). Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, Shane Legg. 2018.
		* [A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning](https://arxiv.org/abs/1011.0686). Stephane Ross, Geoffrey J. Gordon, J. Andrew Bagnell. AISTATS 2011.
		* [Adversarial Examples Are Not Bugs, They Are Features](https://arxiv.org/abs/1905.02175). Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, Aleksander Madry. NeurIPS 2019.
		* [Self-critiquing models for assisting human evaluators](https://arxiv.org/abs/2206.05802). William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, Jan Leike. 2022.
		* [Adversarial Training for High-Stakes Reliability](https://arxiv.org/abs/2205.01663). Daniel M. Ziegler, Seraphina Nix, Lawrence Chan, Tim Bauman, Peter Schmidt-Nielsen, Tao Lin, Adam Scherlis, Noa Nabeshima, Ben Weinstein-Raun, Daniel de Haas, Buck Shlegeris, Nate Thomas. NeurIPS 2022.
					
		
* Week 9: Accountability, auditing, regulation and law
	* May 16, [Accountable Algorithms](https://scholarship.law.upenn.edu/cgi/viewcontent.cgi?article=9570&context=penn_law_review). Joshua A. Kroll, Joanna Huey, Solon Barocas, Edward W. Felten, Joel R. Reidenberg, David G. Robinson, and Harlan Yu. University of Pennsylvenia Law Review. 2017.
	* May 18,  [Closing the AI accountability gap: defining an end-to-end framework for internal algorithmic auditing](https://dl.acm.org/doi/abs/10.1145/3351095.3372873). Inioluwa Deborah Raji, Andrew Smart, Rebecca N. White, Margaret Mitchell, Timnit Gebru, Ben Hutchinson, Jamila Smith-Loud, Daniel Theron, Parker Barnes. FAccT 2020.
	_Additional reading_:
		* [Mitigating bias in algorithmic hiring: evaluating claims and practices](https://dl.acm.org/doi/abs/10.1145/3351095.3372828). Manish Raghavan, Solon Barocas, Jon Kleinberg, Karen Levy. FAccT 2020.
		* [Model Cards for Model Reporting](https://dl.acm.org/doi/abs/10.1145/3287560.3287596). Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, Timnit Gebru. FAccT 2019.
		* [Building and Auditing Fair Algorithms: A Case Study in Candidate Screening](https://dl.acm.org/doi/10.1145/3442188.3445928). Christo Wilson, 
Avijit Ghosh, Shan Jiang, Alan Mislove, Lewis Baker, Janelle Szary, Kelly Trindel, Frida Polli. FAccT 2021.
		* [Actionable Auditing: Investigating the Impact of Publicly Naming Biased Performance Results of Commercial AI Products](https://dl.acm.org/doi/10.1145/3306618.3314244). Inioluwa Deborah Raji and Joy Buolamwini. AIES 2019.
		* [Datasheets for datasets](https://arxiv.org/abs/1803.09010). Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé III, Kate Crawford. 2018.
		*  [European Union regulations on algorithmic decision-making and a "right to explanation"](https://arxiv.org/pdf/1606.08813.pdf). Bryce Goodman and Seth Flaxman. 
		* [Equality of opportunity in supervised learning](https://arxiv.org/abs/1610.02413). Moritz Hardt, Eric Price, Nathan Srebro. NeurIPS 2016.
		* [Inherent Trade-Offs in the Fair Determination of Risk Scores](https://arxiv.org/abs/1609.05807). Jon Kleinberg, Sendhil Mullainathan, Manish Raghavan. ITCS 2017.   
		* [Algorithmic decision making and the cost of fairness](https://arxiv.org/abs/1701.08230). Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, Aziz Huq. KDD 2017. 
		* [Fairness and Abstraction in Sociotechnical Systems](https://dl.acm.org/doi/10.1145/3287560.3287598). Andrew D. Selbst, danah boyd, Sorelle A. Friedler, Suresh Venkatasubramanian, Janet Vertesi. FAT* 2019.   
	
	


* Week 10: Final Project Presentation  
	* TBD, presentations (could be different from normal class time).   
<span style="color:red">**Final project report due on May 26.**</span>
